#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CircuitNet æµ‹è¯•è„šæœ¬
åŸºäºibUNeté¡¹ç›®çš„inference.pyæ”¹å†™
"""

from __future__ import print_function
import os
# åœ¨å¯¼å…¥PyTorchä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç¡®ä¿åªä½¿ç”¨GPU 0
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# æŠ‘åˆ¶å¸¸è§è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", message=".*MMCV will release v2.0.0.*")
warnings.filterwarnings("ignore", message=".*A NumPy version.*is required for this version of SciPy.*")
warnings.filterwarnings("ignore", message=".*Importing from timm.models.layers is deprecated.*")
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import os.path as osp
import json
import numpy as np
import sys
import torch
import gc
from tqdm import tqdm
from datasets.build_dataset import build_dataset
from utils.metrics import build_metric, build_roc_prc_metric
from models.build_model import build_model
from utils.configs import Parser





def test():
    """æµ‹è¯•å‡½æ•°"""
    argp = Parser()
    arg = argp.parser.parse_args()
    
    arg_dict = vars(arg)
    if arg.arg_file is not None:
        with open(arg.arg_file, 'rt') as f:
            arg_dict.update(json.load(f))
    
    arg_dict['ann_file'] = arg_dict['ann_file_test'] 
    arg_dict['test_mode'] = True
    
    print('===> Building model')
    # Initialize model parameters
    model = build_model(arg_dict)

    # è®¾å¤‡é€‰æ‹© - ç¡®ä¿åªä½¿ç”¨å•ä¸ªGPU
    if not arg_dict.get('cpu', False):
        print("\n===> ä½¿ç”¨GPUæµ‹è¯•")

        # ç¡®ä¿CUDAå¯ç”¨å¹¶é€‰æ‹©è®¾å¤‡
        if torch.cuda.is_available():
            device = torch.device('cuda')  # ç”±äºCUDA_VISIBLE_DEVICES='0'ï¼Œè¿™é‡Œä¼šè‡ªåŠ¨ä½¿ç”¨GPU 0
            print(f"âœ… ä½¿ç”¨GPU: {device}")
            print(f"âœ… å¯è§GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"âœ… å½“å‰CUDAè®¾å¤‡: cuda:{torch.cuda.current_device()}")
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")

        model = model.to(device)
        print(f"âœ… æ¨¡å‹ç§»åŠ¨åˆ° {device}")
    else:
        print("\n===> ä½¿ç”¨CPUæµ‹è¯•")
        device = torch.device('cpu')
        model = model.to(device)

    print('\n===> Loading datasets')
    # Initialize dataset
    dataset = build_dataset(arg_dict)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if arg_dict.get('pretrained'):
        print(f"===> åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {arg_dict['pretrained']}")
        if device.type == 'cuda':
            # å¼ºåˆ¶æ˜ å°„åˆ°å½“å‰å¯è§çš„GPUè®¾å¤‡ï¼Œé¿å…è®¾å¤‡ä¸åŒ¹é…é”™è¯¯
            checkpoint = torch.load(arg_dict['pretrained'], map_location=device)
        else:
            checkpoint = torch.load(arg_dict['pretrained'], map_location='cpu')

        # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                print("âœ… æ£€æµ‹åˆ°åŒ…å«state_dictçš„æ£€æŸ¥ç‚¹æ ¼å¼")
            else:
                state_dict = checkpoint
                print("âœ… æ£€æµ‹åˆ°ç›´æ¥state_dictæ ¼å¼")
        else:
            state_dict = checkpoint
            print("âœ… æ£€æµ‹åˆ°å…¶ä»–æ ¼å¼æ£€æŸ¥ç‚¹")

        # å°è¯•åŠ è½½state_dict
        try:
            model.load_state_dict(state_dict, strict=True)
            print("âœ… ä¸¥æ ¼æ¨¡å¼åŠ è½½æˆåŠŸ")
        except RuntimeError as e:
            print(f"âŒ ä¸¥æ ¼æ¨¡å¼åŠ è½½å¤±è´¥: {str(e)[:200]}...")
            print("ğŸ”„ å°è¯•éä¸¥æ ¼æ¨¡å¼åŠ è½½...")
            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
            if missing_keys:
                print(f"âš ï¸  ç¼ºå¤±çš„é”® ({len(missing_keys)}ä¸ª): {missing_keys[:5]}...")
            if unexpected_keys:
                print(f"âš ï¸  æ„å¤–çš„é”® ({len(unexpected_keys)}ä¸ª): {unexpected_keys[:5]}...")
            print("âœ… éä¸¥æ ¼æ¨¡å¼åŠ è½½å®Œæˆ")
    
    model.eval()
    
    # Build metrics
    eval_metrics = arg_dict.get('eval_metric', ['NRMS', 'SSIM','PSNR'])
    metrics = {}
    for k in eval_metrics:
        metric_func = build_metric(k)
        if metric_func is not None:
            metrics[k] = metric_func
        else:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•æ‰¾åˆ°æŒ‡æ ‡å‡½æ•° '{k}'")

    avg_metrics = {k: 0 for k in metrics.keys()}
    
    count = 0
    print(f"\n===> å¼€å§‹æµ‹è¯• ({len(dataset)} ä¸ªæ ·æœ¬)")
    
    with torch.no_grad():
        with tqdm(total=len(dataset)) as bar:
            for feature, label, label_path in dataset:
                if device.type == 'cuda':
                    input, target = feature.to(device), label.to(device)
                else:
                    input, target = feature, label
                
                prediction = model(input)
                
                # è®¡ç®—æŒ‡æ ‡
                for metric, metric_func in metrics.items():
                    try:
                        if metric.lower() in ['peak_nrms', 'peaknrms']:
                            # å¯¹äºpeak_nrmsï¼Œç›´æ¥ä¼ é€’tensorï¼Œè®©è£…é¥°å™¨å¤„ç†è½¬æ¢
                            detailed_result = metric_func(target.cpu(), prediction.squeeze(1).cpu(), return_dict=True)
                            avg_value = metric_func(target.cpu(), prediction.squeeze(1).cpu(), return_dict=False)

                            # å­˜å‚¨è¯¦ç»†ç»“æœ
                            if 'peak_nrms_detailed' not in avg_metrics:
                                avg_metrics['peak_nrms_detailed'] = {p: 0 for p in [0.5, 1, 2, 5]}

                            for p, value in detailed_result.items():
                                avg_metrics['peak_nrms_detailed'][p] += value

                            avg_metrics[metric] += avg_value
                        else:
                            metric_value = metric_func(target.cpu(), prediction.squeeze(1).cpu())
                            if metric_value != 1:  # é¿å…å¼‚å¸¸å€¼
                                avg_metrics[metric] += metric_value
                    except Exception as e:
                        print(f"âš ï¸  æŒ‡æ ‡ {metric} è®¡ç®—å¤±è´¥: {e}")
                
                # ä¿å­˜æµ‹è¯•ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
                if arg_dict.get('plot_roc', False):
                    save_path = osp.join(arg_dict['save_path'], 'test_result')
                    if not os.path.exists(save_path):
                        os.makedirs(save_path)
                    file_name = osp.splitext(osp.basename(label_path[0]))[0]
                    save_file = osp.join(save_path, f'{file_name}.npy')
                    output_final = prediction.float().detach().cpu().numpy()
                    np.save(save_file, output_final)
                    count += 1
                
                bar.update(1)
    
    # è¾“å‡ºå¹³å‡æŒ‡æ ‡
    print("\n===> æµ‹è¯•ç»“æœ:")

    if len(dataset) == 0:
        print("  âš ï¸  æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡")
        return

    # åŸºç¡€æŒ‡æ ‡
    basic_metrics = ['NRMS', 'SSIM', 'PSNR', 'MAE']
    for metric in basic_metrics:
        if metric in avg_metrics:
            avg_value = avg_metrics[metric] / len(dataset)
            print(f"  {metric}: {avg_value:.4f}")

    # Peak NRMSè¯¦ç»†ç»“æœï¼ˆç±»ä¼¼æ‚¨çš„è¡¨æ ¼æ ¼å¼ï¼‰
    if 'peak_nrms_detailed' in avg_metrics:
        print(f"\n  Peak NRMSEè¯¦ç»†ç»“æœ:")
        percentiles = [0.5, 1, 2, 5]
        header = "    " + "".join([f"{p:>8}%" for p in percentiles]) + f"{'average':>10}"
        print(header)

        values = []
        for p in percentiles:
            avg_val = avg_metrics['peak_nrms_detailed'][p] / len(dataset)
            values.append(avg_val)

        avg_all = sum(values) / len(values)
        values_str = "    " + "".join([f"{v:8.3f}" for v in values]) + f"{avg_all:10.3f}"
        print(values_str)

    # ç›¸å…³æ€§æŒ‡æ ‡
    correlation_metrics = ['R2', 'PEARSON', 'SPEARMAN', 'KENDALL']
    correlation_found = False
    for metric in correlation_metrics:
        if metric in avg_metrics:
            if not correlation_found:
                print(f"\n  ç›¸å…³æ€§æŒ‡æ ‡:")
                correlation_found = True
            avg_value = avg_metrics[metric] / len(dataset)
            print(f"    {metric}: {avg_value:.4f}")

    # å…¶ä»–æŒ‡æ ‡
    other_metrics = ['AUC']
    for metric in other_metrics:
        if metric in avg_metrics:
            avg_value = avg_metrics[metric] / len(dataset)
            print(f"  {metric}: {avg_value:.4f}")

    # æ˜¾ç¤ºæ‰€æœ‰å…¶ä»–æœªåˆ†ç±»çš„æŒ‡æ ‡
    displayed_metrics = set(basic_metrics + ['peak_nrms_detailed'] + correlation_metrics + other_metrics)
    for metric, avg_metric in avg_metrics.items():
        if metric not in displayed_metrics:
            avg_value = avg_metric / len(dataset)
            print(f"  {metric}: {avg_value:.4f}")
    
    # ROCåˆ†æï¼ˆå¦‚æœéœ€è¦ï¼‰
    if arg_dict.get('plot_roc', False):
        try:
            roc_metric, _ = build_roc_prc_metric(**arg_dict)
            print(f"\n===> AUC of ROC: {roc_metric:.4f}")
        except Exception as e:
            print(f"âš ï¸  ROCåˆ†æå¤±è´¥: {e}")
    
    print(f"\n===> æµ‹è¯•å®Œæˆï¼Œå…±å¤„ç† {len(dataset)} ä¸ªæ ·æœ¬")
    if count > 0:
        print(f"===> ä¿å­˜äº† {count} ä¸ªé¢„æµ‹ç»“æœ")


if __name__ == "__main__":
    test()
